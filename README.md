# AutoProg

This project is used to update news related to our paper [***Automated Progressive Learning for Efficient Training of Vision Transformers***](https://arxiv.org/pdf/2203.14509) (CVPR 2022).

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/61453811/160517880-c6349005-a2ab-4587-a51f-644cf4a7360e.png" width=95%/></p>
<p align="center">
AutoProg achieves efficient training by automatically increasing the training overload on-the-fly.</p>

<p align="center">
<img alt="image" src="https://user-images.githubusercontent.com/61453811/160518329-1f4798f8-e423-4f3a-add4-9369c3fcc5c0.png" width=95%/></p>
<p align="center">
AutoProg can accelerate ViTs training by up to 85.1% with no performance drop.
</p>

## Implementations 

Code will be released soon.

## Citation
If you use our code for your paper, please cite:
```bibtex
@inproceedings{li2022autoprog,
  author = {Li, Changlin and 
            Zhuang, Bohan and 
            Wang, Guangrun and
            Liang, Xiaodan and
            Chang, Xiaojun and
            Yang, Yi},
  title = {Automated Progressive Learning for Efficient Training of Vision Transformers},
  booktitle = {CVPR},
  year = 2022,
}
```
